{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“ˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ø´ÛŒÙ† Ù„Ø±Ù†ÛŒÙ†Ú¯ - Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÙˆØ³ØªÙˆÙ†\n",
    "\n",
    "Ø§ÛŒÙ† Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ø§Ù…Ù„ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ­Ù„ÛŒÙ„ Ø¹Ù…ÛŒÙ‚ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ Ø§Ø³Øª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data and models\n",
    "from data_loader import BostonHousingDataLoader\n",
    "from preprocessing import DataPreprocessor\n",
    "from models import ModelTrainer\n",
    "from evaluation import ModelEvaluator\n",
    "\n",
    "# Load data\n",
    "loader = BostonHousingDataLoader()\n",
    "features, target, feature_names = loader.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "preprocessor = DataPreprocessor(scaler_type='standard')\n",
    "features_clean = preprocessor.handle_outliers(features, strategy='clip')\n",
    "X_train, X_test, y_train, y_test = preprocessor.split_data(features_clean, target)\n",
    "X_train_scaled, X_test_scaled = preprocessor.scale_features(X_train, X_test)\n",
    "\n",
    "# Train models\n",
    "trainer = ModelTrainer()\n",
    "results = trainer.train_models(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f\"ğŸ“Š Data and models loaded successfully!\")\n",
    "print(f\"Models trained: {len(results)}\")\n",
    "print(f\"Best model: {trainer.best_model.__class__.__name__ if trainer.best_model else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"ğŸ“Š Evaluating all models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    y_pred = result['y_pred']\n",
    "    metrics = evaluator.calculate_metrics(y_test, y_pred, model_name)\n",
    "    evaluator.print_metrics(model_name)\n",
    "\n",
    "print(f\"âœ… Evaluation completed for {len(results)} models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ vs ÙˆØ§Ù‚Ø¹ÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for best model\n",
    "if trainer.best_model:\n",
    "    best_model_name = None\n",
    "    for name, model in trainer.models.items():\n",
    "        if model == trainer.best_model:\n",
    "            best_model_name = name\n",
    "            break\n",
    "    \n",
    "    if best_model_name:\n",
    "        print(f\"ğŸ“ˆ Creating prediction plots for best model: {best_model_name}\")\n",
    "        y_pred_best = results[best_model_name]['y_pred']\n",
    "        evaluator.plot_predictions_vs_actual(y_test, y_pred_best, best_model_name)\n",
    "    else:\n",
    "        print(\"âŒ Could not identify best model name!\")\n",
    "else:\n",
    "    print(\"âŒ No best model available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ØªØ­Ù„ÛŒÙ„ Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive residual analysis for best model\n",
    "if trainer.best_model and best_model_name:\n",
    "    print(f\"ğŸ” Performing residual analysis for {best_model_name}...\")\n",
    "    y_pred_best = results[best_model_name]['y_pred']\n",
    "    evaluator.plot_residual_analysis(y_test, y_pred_best, best_model_name)\n",
    "else:\n",
    "    print(\"âŒ No best model available for residual analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for best model\n",
    "if trainer.best_model and best_model_name:\n",
    "    print(f\"ğŸ“Š Plotting learning curves for {best_model_name}...\")\n",
    "    evaluator.plot_learning_curves(trainer.best_model, X_train_scaled, y_train)\n",
    "else:\n",
    "    print(\"âŒ No best model available for learning curves!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Validation Curves Ø¨Ø±Ø§ÛŒ Ù‡ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation curves for key parameters\n",
    "if 'Random Forest' in trainer.models:\n",
    "    print(\"âš™ï¸ Plotting validation curves for Random Forest...\")\n",
    "    \n",
    "    # n_estimators validation curve\n",
    "    n_estimators_range = [10, 25, 50, 100, 200]\n",
    "    evaluator.plot_validation_curves(\n",
    "        trainer.models['Random Forest'], \n",
    "        X_train_scaled, y_train, \n",
    "        'n_estimators', n_estimators_range\n",
    "    )\n",
    "    \n",
    "    # max_depth validation curve\n",
    "    max_depth_range = [3, 5, 10, 15, 20, None]\n",
    "    evaluator.plot_validation_curves(\n",
    "        trainer.models['Random Forest'], \n",
    "        X_train_scaled, y_train, \n",
    "        'max_depth', max_depth_range\n",
    "    )\n",
    "else:\n",
    "    print(\"âš ï¸ Random Forest not available for validation curves!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¬Ø§Ù…Ø¹ Ù…Ø¯Ù„â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models comprehensively\n",
    "print(\"ğŸ“Š Creating comprehensive model comparison...\")\n",
    "evaluator.compare_models(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ØªØ­Ù„ÛŒÙ„ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed performance analysis\n",
    "performance_analysis = {}\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    performance_analysis[model_name] = {\n",
    "        'RÂ²': result['r2'],\n",
    "        'RMSE': result['rmse'],\n",
    "        'MAE': result['mae'],\n",
    "        'CV_RÂ²_Mean': result['cv_mean'],\n",
    "        'CV_RÂ²_Std': result['cv_std']\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "performance_df = pd.DataFrame(performance_analysis).T\n",
    "\n",
    "print(\"ğŸ¯ Detailed Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "display(performance_df.round(4))\n",
    "\n",
    "# Performance rankings\n",
    "print(\"\\nğŸ† Performance Rankings:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# RÂ² ranking\n",
    "r2_ranking = performance_df['RÂ²'].sort_values(ascending=False)\n",
    "print(\"RÂ² Score Ranking:\")\n",
    "for i, (model, score) in enumerate(r2_ranking.items(), 1):\n",
    "    print(f\"  {i}. {model}: {score:.4f}\")\n",
    "\n",
    "# RMSE ranking\n",
    "rmse_ranking = performance_df['RMSE'].sort_values()\n",
    "print(\"\\nRMSE Ranking (Lower is better):\")\n",
    "for i, (model, score) in enumerate(rmse_ranking.items(), 1):\n",
    "    print(f\"  {i}. {model}: {score:.4f}\")\n",
    "\n",
    "# CV RÂ² ranking\n",
    "cv_r2_ranking = performance_df['CV_RÂ²_Mean'].sort_values(ascending=False)\n",
    "print(\"\\nCross-Validation RÂ² Ranking:\")\n",
    "for i, (model, score) in enumerate(cv_r2_ranking.items(), 1):\n",
    "    print(f\"  {i}. {model}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 1. RÂ² scores\n",
    "models = list(performance_df.index)\n",
    "r2_scores = performance_df['RÂ²'].values\n",
    "bars1 = axes[0].bar(models, r2_scores, color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('RÂ² Scores Comparison')\n",
    "axes[0].set_ylabel('RÂ² Score')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars1, r2_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. RMSE scores\n",
    "rmse_scores = performance_df['RMSE'].values\n",
    "bars2 = axes[1].bar(models, rmse_scores, color='lightcoral', alpha=0.7)\n",
    "axes[1].set_title('RMSE Scores Comparison')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars2, rmse_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. CV RÂ² scores\n",
    "cv_r2_scores = performance_df['CV_RÂ²_Mean'].values\n",
    "cv_r2_stds = performance_df['CV_RÂ²_Std'].values\n",
    "bars3 = axes[2].bar(models, cv_r2_scores, yerr=cv_r2_stds, \n",
    "                    color='lightgreen', alpha=0.7, capsize=5)\n",
    "axes[2].set_title('Cross-Validation RÂ² Scores')\n",
    "axes[2].set_ylabel('CV RÂ² Score')\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars3, cv_r2_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. MAE scores\n",
    "mae_scores = performance_df['MAE'].values\n",
    "bars4 = axes[3].bar(models, mae_scores, color='orange', alpha=0.7)\n",
    "axes[3].set_title('MAE Scores Comparison')\n",
    "axes[3].set_ylabel('MAE')\n",
    "axes[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars4, mae_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[3].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'XGBoost', 'Gradient Boosting', 'Decision Tree']\n",
    "feature_importance_summary = {}\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in trainer.models:\n",
    "        importance = trainer.get_feature_importance(model_name, feature_names)\n",
    "        if importance:\n",
    "            feature_importance_summary[model_name] = importance\n",
    "            print(f\"âœ… Feature importance obtained for {model_name}\")\n",
    "        else:\n",
    "            print(f\"âŒ Could not get feature importance for {model_name}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {model_name} not found in trained models\")\n",
    "\n",
    "# Compare feature importance across models\n",
    "if feature_importance_summary:\n",
    "    print(f\"\\nğŸ” Feature Importance Comparison:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get top 5 features from each model\n",
    "    for model_name, importance in feature_importance_summary.items():\n",
    "        print(f\"\\n{model_name} - Top 5 Features:\")\n",
    "        top_5 = list(importance.items())[:5]\n",
    "        for i, (feature, score) in enumerate(top_5, 1):\n",
    "            print(f\"  {i}. {feature}: {score:.4f}\")\n",
    "    \n",
    "    # Find common important features\n",
    "    all_features = set()\n",
    "    for importance in feature_importance_summary.values():\n",
    "        all_features.update(list(importance.keys())[:5])\n",
    "    \n",
    "    print(f\"\\nğŸ”— Common important features across models: {len(all_features)}\")\n",
    "    for feature in sorted(all_features):\n",
    "        print(f\"  - {feature}\")\n",
    "else:\n",
    "    print(\"âŒ No feature importance data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for all tree-based models\n",
    "if feature_importance_summary:\n",
    "    n_models = len(feature_importance_summary)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (model_name, importance) in enumerate(feature_importance_summary.items()):\n",
    "        if i < len(axes):\n",
    "            # Get top 10 features\n",
    "            top_features = dict(list(importance.items())[:10])\n",
    "            features = list(top_features.keys())\n",
    "            scores = list(top_features.values())\n",
    "            \n",
    "            y_pos = np.arange(len(features))\n",
    "            axes[i].barh(y_pos, scores, color='lightgreen', alpha=0.7)\n",
    "            axes[i].set_yticks(y_pos)\n",
    "            axes[i].set_yticklabels(features)\n",
    "            axes[i].set_xlabel('Feature Importance')\n",
    "            axes[i].set_title(f'Top 10 Feature Importance - {model_name}')\n",
    "            axes[i].invert_yaxis()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for j, score in enumerate(scores):\n",
    "                axes[i].text(score + 0.001, j, f'{score:.3f}', va='center')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(feature_importance_summary), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"âŒ No feature importance plots to show!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Ú¯Ø²Ø§Ø±Ø´ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¬Ø§Ù…Ø¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation report\n",
    "print(\"ğŸ“‹ Creating comprehensive evaluation report...\")\n",
    "evaluator.create_evaluation_report('../results/evaluation_report.txt')\n",
    "print(\"âœ… Evaluation report created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ØªØ­Ù„ÛŒÙ„ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final analysis and recommendations\n",
    "print(\"ğŸ¯ Final Analysis and Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Best model analysis\n",
    "if trainer.best_model:\n",
    "    best_model_name = None\n",
    "    for name, model in trainer.models.items():\n",
    "        if model == trainer.best_model:\n",
    "            best_model_name = name\n",
    "            break\n",
    "    \n",
    "    if best_model_name:\n",
    "        best_result = results[best_model_name]\n",
    "        print(f\"ğŸ† Best Model: {best_model_name}\")\n",
    "        print(f\"   RÂ² Score: {best_result['r2']:.4f}\")\n",
    "        print(f\"   RMSE: {best_result['rmse']:.4f}\")\n",
    "        print(f\"   MAE: {best_result['mae']:.4f}\")\n",
    "        print(f\"   CV RÂ²: {best_result['cv_mean']:.4f} Â± {best_result['cv_std']:.4f}\")\n",
    "        \n",
    "        # Performance category\n",
    "        r2_score = best_result['r2']\n",
    "        if r2_score >= 0.8:\n",
    "            performance = \"Excellent ğŸ¯\"\n",
    "            recommendation = \"Model is ready for production use\"\n",
    "        elif r2_score >= 0.6:\n",
    "            performance = \"Good ğŸ‘\"\n",
    "            recommendation = \"Model performs well, consider feature engineering\"\n",
    "        elif r2_score >= 0.4:\n",
    "            performance = \"Fair âš ï¸\"\n",
    "            recommendation = \"Model needs improvement, try different algorithms\"\n",
    "        else:\n",
    "            performance = \"Poor âŒ\"\n",
    "            recommendation = \"Model needs significant improvement\"\n",
    "        \n",
    "        print(f\"   Performance: {performance}\")\n",
    "        print(f\"   Recommendation: {recommendation}\")\n",
    "    else:\n",
    "        print(\"âŒ Could not identify best model!\")\n",
    "else:\n",
    "    print(\"âŒ No best model available!\")\n",
    "\n",
    "# Model stability analysis\n",
    "print(f\"\\nğŸ” Model Stability Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "cv_stability = {}\n",
    "for model_name, result in results.items():\n",
    "    cv_std = result['cv_std']\n",
    "    if cv_std < 0.05:\n",
    "        stability = \"Very Stable ğŸŸ¢\"\n",
    "    elif cv_std < 0.1:\n",
    "        stability = \"Stable ğŸŸ¡\"\n",
    "    else:\n",
    "        stability = \"Unstable ğŸ”´\"\n",
    "    \n",
    "    cv_stability[model_name] = {\n",
    "        'cv_std': cv_std,\n",
    "        'stability': stability\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name}: {stability} (CV Std: {cv_std:.4f})\")\n",
    "\n",
    "# Feature importance insights\n",
    "if feature_importance_summary:\n",
    "    print(f\"\\nğŸ” Feature Importance Insights:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Find most important features across all models\n",
    "    feature_scores = {}\n",
    "    for model_name, importance in feature_importance_summary.items():\n",
    "        for feature, score in importance.items():\n",
    "            if feature not in feature_scores:\n",
    "                feature_scores[feature] = []\n",
    "            feature_scores[feature].append(score)\n",
    "    \n",
    "    # Calculate average importance\n",
    "    avg_importance = {}\n",
    "    for feature, scores in feature_scores.items():\n",
    "        avg_importance[feature] = np.mean(scores)\n",
    "    \n",
    "    # Show top 10 most important features\n",
    "    top_features_avg = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"Top 10 Most Important Features (Average across models):\")\n",
    "    for i, (feature, avg_score) in enumerate(top_features_avg, 1):\n",
    "        print(f\"  {i}. {feature}: {avg_score:.4f}\")\n",
    "\n",
    "# Final recommendations\n",
    "print(f\"\\nğŸ’¡ Final Recommendations:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. Use the best performing model for predictions\")\n",
    "print(\"2. Consider ensemble methods for improved performance\")\n",
    "print(\"3. Focus on the most important features for feature engineering\")\n",
    "print(\"4. Monitor model performance on new data\")\n",
    "print(\"5. Regular retraining with updated data\")\n",
    "print(\"6. Consider business context when interpreting results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "import json\n",
    "\n",
    "evaluation_summary = {\n",
    "    'best_model': {\n",
    "        'name': best_model_name if best_model_name else 'None',\n",
    "        'r2_score': float(best_result['r2']) if best_model_name else 0.0,\n",
    "        'rmse': float(best_result['rmse']) if best_model_name else 0.0,\n",
    "        'mae': float(best_result['mae']) if best_model_name else 0.0\n",
    "    },\n",
    "    'model_performance': performance_analysis,\n",
    "    'cv_stability': cv_stability,\n",
    "    'feature_importance_summary': {}\n",
    "}\n",
    "\n",
    "# Add feature importance data\n",
    "if feature_importance_summary:\n",
    "    for model_name, importance in feature_importance_summary.items():\n",
    "        evaluation_summary['feature_importance_summary'][model_name] = {\n",
    "            feature: float(score) for feature, score in importance.items()\n",
    "        }\n",
    "\n",
    "with open('../results/evaluation_summary.json', 'w') as f:\n",
    "    json.dump(evaluation_summary, f, indent=2)\n",
    "\n",
    "print(\"ğŸ’¾ Evaluation results saved to 'results/evaluation_summary.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ù†Ù‡Ø§ÛŒÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ Final Evaluation Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Models evaluated: {len(results)}\")\n",
    "print(f\"Best model: {best_model_name if best_model_name else 'None'}\")\n",
    "print(f\"Best RÂ² score: {trainer.best_score:.4f if trainer.best_score else 'N/A'}\")\n",
    "print(f\"Feature importance analyzed: {len(feature_importance_summary)} models\")\n",
    "print(f\"Evaluation report created: âœ…\")\n",
    "print(f\"Residual analysis performed: âœ…\")\n",
    "print(f\"Learning curves generated: âœ…\")\n",
    "print(f\"Model comparison completed: âœ…\")\n",
    "\n",
    "print(f\"\\nğŸ“ Generated Files:\")\n",
    "print(f\"  - results/evaluation_report.txt\")\n",
    "print(f\"  - results/evaluation_summary.json\")\n",
    "print(f\"  - Various evaluation plots and visualizations\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Comprehensive evaluation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
     "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.5"
   }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
