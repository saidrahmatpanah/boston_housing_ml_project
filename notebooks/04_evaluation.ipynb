{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📈 ارزیابی جامع مدل‌های ماشین لرنینگ - خانه‌های بوستون\n",
    "\n",
    "این نوت‌بوک شامل ارزیابی جامع و تحلیل عمیق عملکرد مدل‌های آموزش دیده است."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 بارگذاری مدل‌ها و داده‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data and models\n",
    "from data_loader import BostonHousingDataLoader\n",
    "from preprocessing import DataPreprocessor\n",
    "from models import ModelTrainer\n",
    "from evaluation import ModelEvaluator\n",
    "\n",
    "# Load data\n",
    "loader = BostonHousingDataLoader()\n",
    "features, target, feature_names = loader.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "preprocessor = DataPreprocessor(scaler_type='standard')\n",
    "features_clean = preprocessor.handle_outliers(features, strategy='clip')\n",
    "X_train, X_test, y_train, y_test = preprocessor.split_data(features_clean, target)\n",
    "X_train_scaled, X_test_scaled = preprocessor.scale_features(X_train, X_test)\n",
    "\n",
    "# Train models\n",
    "trainer = ModelTrainer()\n",
    "results = trainer.train_models(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f\"📊 Data and models loaded successfully!\")\n",
    "print(f\"Models trained: {len(results)}\")\n",
    "print(f\"Best model: {trainer.best_model.__class__.__name__ if trainer.best_model else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 ارزیابی جامع مدل‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator()\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"📊 Evaluating all models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    y_pred = result['y_pred']\n",
    "    metrics = evaluator.calculate_metrics(y_test, y_pred, model_name)\n",
    "    evaluator.print_metrics(model_name)\n",
    "\n",
    "print(f\"✅ Evaluation completed for {len(results)} models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 نمودارهای پیش‌بینی vs واقعی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actual for best model\n",
    "if trainer.best_model:\n",
    "    best_model_name = None\n",
    "    for name, model in trainer.models.items():\n",
    "        if model == trainer.best_model:\n",
    "            best_model_name = name\n",
    "            break\n",
    "    \n",
    "    if best_model_name:\n",
    "        print(f\"📈 Creating prediction plots for best model: {best_model_name}\")\n",
    "        y_pred_best = results[best_model_name]['y_pred']\n",
    "        evaluator.plot_predictions_vs_actual(y_test, y_pred_best, best_model_name)\n",
    "    else:\n",
    "        print(\"❌ Could not identify best model name!\")\n",
    "else:\n",
    "    print(\"❌ No best model available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 تحلیل Residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive residual analysis for best model\n",
    "if trainer.best_model and best_model_name:\n",
    "    print(f\"🔍 Performing residual analysis for {best_model_name}...\")\n",
    "    y_pred_best = results[best_model_name]['y_pred']\n",
    "    evaluator.plot_residual_analysis(y_test, y_pred_best, best_model_name)\n",
    "else:\n",
    "    print(\"❌ No best model available for residual analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 نمودارهای Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curves for best model\n",
    "if trainer.best_model and best_model_name:\n",
    "    print(f\"📊 Plotting learning curves for {best_model_name}...\")\n",
    "    evaluator.plot_learning_curves(trainer.best_model, X_train_scaled, y_train)\n",
    "else:\n",
    "    print(\"❌ No best model available for learning curves!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ Validation Curves برای هیپرپارامترها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation curves for key parameters\n",
    "if 'Random Forest' in trainer.models:\n",
    "    print(\"⚙️ Plotting validation curves for Random Forest...\")\n",
    "    \n",
    "    # n_estimators validation curve\n",
    "    n_estimators_range = [10, 25, 50, 100, 200]\n",
    "    evaluator.plot_validation_curves(\n",
    "        trainer.models['Random Forest'], \n",
    "        X_train_scaled, y_train, \n",
    "        'n_estimators', n_estimators_range\n",
    "    )\n",
    "    \n",
    "    # max_depth validation curve\n",
    "    max_depth_range = [3, 5, 10, 15, 20, None]\n",
    "    evaluator.plot_validation_curves(\n",
    "        trainer.models['Random Forest'], \n",
    "        X_train_scaled, y_train, \n",
    "        'max_depth', max_depth_range\n",
    "    )\n",
    "else:\n",
    "    print(\"⚠️ Random Forest not available for validation curves!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 مقایسه جامع مدل‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models comprehensively\n",
    "print(\"📊 Creating comprehensive model comparison...\")\n",
    "evaluator.compare_models(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 تحلیل عملکرد بر اساس معیارهای مختلف"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create detailed performance analysis\n",
    "performance_analysis = {}\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    performance_analysis[model_name] = {\n",
    "        'R²': result['r2'],\n",
    "        'RMSE': result['rmse'],\n",
    "        'MAE': result['mae'],\n",
    "        'CV_R²_Mean': result['cv_mean'],\n",
    "        'CV_R²_Std': result['cv_std']\n",
    "    }\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "performance_df = pd.DataFrame(performance_analysis).T\n",
    "\n",
    "print(\"🎯 Detailed Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "display(performance_df.round(4))\n",
    "\n",
    "# Performance rankings\n",
    "print(\"\\n🏆 Performance Rankings:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# R² ranking\n",
    "r2_ranking = performance_df['R²'].sort_values(ascending=False)\n",
    "print(\"R² Score Ranking:\")\n",
    "for i, (model, score) in enumerate(r2_ranking.items(), 1):\n",
    "    print(f\"  {i}. {model}: {score:.4f}\")\n",
    "\n",
    "# RMSE ranking\n",
    "rmse_ranking = performance_df['RMSE'].sort_values()\n",
    "print(\"\\nRMSE Ranking (Lower is better):\")\n",
    "for i, (model, score) in enumerate(rmse_ranking.items(), 1):\n",
    "    print(f\"  {i}. {model}: {score:.4f}\")\n",
    "\n",
    "# CV R² ranking\n",
    "cv_r2_ranking = performance_df['CV_R²_Mean'].sort_values(ascending=False)\n",
    "print(\"\\nCross-Validation R² Ranking:\")\n",
    "for i, (model, score) in enumerate(cv_r2_ranking.items(), 1):\n",
    "    print(f\"  {i}. {model}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 نمودارهای عملکرد"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# 1. R² scores\n",
    "models = list(performance_df.index)\n",
    "r2_scores = performance_df['R²'].values\n",
    "bars1 = axes[0].bar(models, r2_scores, color='skyblue', alpha=0.7)\n",
    "axes[0].set_title('R² Scores Comparison')\n",
    "axes[0].set_ylabel('R² Score')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars1, r2_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 2. RMSE scores\n",
    "rmse_scores = performance_df['RMSE'].values\n",
    "bars2 = axes[1].bar(models, rmse_scores, color='lightcoral', alpha=0.7)\n",
    "axes[1].set_title('RMSE Scores Comparison')\n",
    "axes[1].set_ylabel('RMSE')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars2, rmse_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. CV R² scores\n",
    "cv_r2_scores = performance_df['CV_R²_Mean'].values\n",
    "cv_r2_stds = performance_df['CV_R²_Std'].values\n",
    "bars3 = axes[2].bar(models, cv_r2_scores, yerr=cv_r2_stds, \n",
    "                    color='lightgreen', alpha=0.7, capsize=5)\n",
    "axes[2].set_title('Cross-Validation R² Scores')\n",
    "axes[2].set_ylabel('CV R² Score')\n",
    "axes[2].set_ylim(0, 1)\n",
    "axes[2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars3, cv_r2_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 4. MAE scores\n",
    "mae_scores = performance_df['MAE'].values\n",
    "bars4 = axes[3].bar(models, mae_scores, color='orange', alpha=0.7)\n",
    "axes[3].set_title('MAE Scores Comparison')\n",
    "axes[3].set_ylabel('MAE')\n",
    "axes[3].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars4, mae_scores):\n",
    "    height = bar.get_height()\n",
    "    axes[3].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 تحلیل ویژگی‌های مهم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'XGBoost', 'Gradient Boosting', 'Decision Tree']\n",
    "feature_importance_summary = {}\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in trainer.models:\n",
    "        importance = trainer.get_feature_importance(model_name, feature_names)\n",
    "        if importance:\n",
    "            feature_importance_summary[model_name] = importance\n",
    "            print(f\"✅ Feature importance obtained for {model_name}\")\n",
    "        else:\n",
    "            print(f\"❌ Could not get feature importance for {model_name}\")\n",
    "    else:\n",
    "        print(f\"⚠️ {model_name} not found in trained models\")\n",
    "\n",
    "# Compare feature importance across models\n",
    "if feature_importance_summary:\n",
    "    print(f\"\\n🔍 Feature Importance Comparison:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get top 5 features from each model\n",
    "    for model_name, importance in feature_importance_summary.items():\n",
    "        print(f\"\\n{model_name} - Top 5 Features:\")\n",
    "        top_5 = list(importance.items())[:5]\n",
    "        for i, (feature, score) in enumerate(top_5, 1):\n",
    "            print(f\"  {i}. {feature}: {score:.4f}\")\n",
    "    \n",
    "    # Find common important features\n",
    "    all_features = set()\n",
    "    for importance in feature_importance_summary.values():\n",
    "        all_features.update(list(importance.keys())[:5])\n",
    "    \n",
    "    print(f\"\\n🔗 Common important features across models: {len(all_features)}\")\n",
    "    for feature in sorted(all_features):\n",
    "        print(f\"  - {feature}\")\n",
    "else:\n",
    "    print(\"❌ No feature importance data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 نمودارهای Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for all tree-based models\n",
    "if feature_importance_summary:\n",
    "    n_models = len(feature_importance_summary)\n",
    "    n_cols = 2\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (model_name, importance) in enumerate(feature_importance_summary.items()):\n",
    "        if i < len(axes):\n",
    "            # Get top 10 features\n",
    "            top_features = dict(list(importance.items())[:10])\n",
    "            features = list(top_features.keys())\n",
    "            scores = list(top_features.values())\n",
    "            \n",
    "            y_pos = np.arange(len(features))\n",
    "            axes[i].barh(y_pos, scores, color='lightgreen', alpha=0.7)\n",
    "            axes[i].set_yticks(y_pos)\n",
    "            axes[i].set_yticklabels(features)\n",
    "            axes[i].set_xlabel('Feature Importance')\n",
    "            axes[i].set_title(f'Top 10 Feature Importance - {model_name}')\n",
    "            axes[i].invert_yaxis()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for j, score in enumerate(scores):\n",
    "                axes[i].text(score + 0.001, j, f'{score:.3f}', va='center')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(feature_importance_summary), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ No feature importance plots to show!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 گزارش ارزیابی جامع"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive evaluation report\n",
    "print(\"📋 Creating comprehensive evaluation report...\")\n",
    "evaluator.create_evaluation_report('../results/evaluation_report.txt')\n",
    "print(\"✅ Evaluation report created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 تحلیل نهایی و توصیه‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final analysis and recommendations\n",
    "print(\"🎯 Final Analysis and Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Best model analysis\n",
    "if trainer.best_model:\n",
    "    best_model_name = None\n",
    "    for name, model in trainer.models.items():\n",
    "        if model == trainer.best_model:\n",
    "            best_model_name = name\n",
    "            break\n",
    "    \n",
    "    if best_model_name:\n",
    "        best_result = results[best_model_name]\n",
    "        print(f\"🏆 Best Model: {best_model_name}\")\n",
    "        print(f\"   R² Score: {best_result['r2']:.4f}\")\n",
    "        print(f\"   RMSE: {best_result['rmse']:.4f}\")\n",
    "        print(f\"   MAE: {best_result['mae']:.4f}\")\n",
    "        print(f\"   CV R²: {best_result['cv_mean']:.4f} ± {best_result['cv_std']:.4f}\")\n",
    "        \n",
    "        # Performance category\n",
    "        r2_score = best_result['r2']\n",
    "        if r2_score >= 0.8:\n",
    "            performance = \"Excellent 🎯\"\n",
    "            recommendation = \"Model is ready for production use\"\n",
    "        elif r2_score >= 0.6:\n",
    "            performance = \"Good 👍\"\n",
    "            recommendation = \"Model performs well, consider feature engineering\"\n",
    "        elif r2_score >= 0.4:\n",
    "            performance = \"Fair ⚠️\"\n",
    "            recommendation = \"Model needs improvement, try different algorithms\"\n",
    "        else:\n",
    "            performance = \"Poor ❌\"\n",
    "            recommendation = \"Model needs significant improvement\"\n",
    "        \n",
    "        print(f\"   Performance: {performance}\")\n",
    "        print(f\"   Recommendation: {recommendation}\")\n",
    "    else:\n",
    "        print(\"❌ Could not identify best model!\")\n",
    "else:\n",
    "    print(\"❌ No best model available!\")\n",
    "\n",
    "# Model stability analysis\n",
    "print(f\"\\n🔍 Model Stability Analysis:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "cv_stability = {}\n",
    "for model_name, result in results.items():\n",
    "    cv_std = result['cv_std']\n",
    "    if cv_std < 0.05:\n",
    "        stability = \"Very Stable 🟢\"\n",
    "    elif cv_std < 0.1:\n",
    "        stability = \"Stable 🟡\"\n",
    "    else:\n",
    "        stability = \"Unstable 🔴\"\n",
    "    \n",
    "    cv_stability[model_name] = {\n",
    "        'cv_std': cv_std,\n",
    "        'stability': stability\n",
    "    }\n",
    "    \n",
    "    print(f\"{model_name}: {stability} (CV Std: {cv_std:.4f})\")\n",
    "\n",
    "# Feature importance insights\n",
    "if feature_importance_summary:\n",
    "    print(f\"\\n🔍 Feature Importance Insights:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Find most important features across all models\n",
    "    feature_scores = {}\n",
    "    for model_name, importance in feature_importance_summary.items():\n",
    "        for feature, score in importance.items():\n",
    "            if feature not in feature_scores:\n",
    "                feature_scores[feature] = []\n",
    "            feature_scores[feature].append(score)\n",
    "    \n",
    "    # Calculate average importance\n",
    "    avg_importance = {}\n",
    "    for feature, scores in feature_scores.items():\n",
    "        avg_importance[feature] = np.mean(scores)\n",
    "    \n",
    "    # Show top 10 most important features\n",
    "    top_features_avg = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"Top 10 Most Important Features (Average across models):\")\n",
    "    for i, (feature, avg_score) in enumerate(top_features_avg, 1):\n",
    "        print(f\"  {i}. {feature}: {avg_score:.4f}\")\n",
    "\n",
    "# Final recommendations\n",
    "print(f\"\\n💡 Final Recommendations:\")\n",
    "print(\"-\" * 30)\n",
    "print(\"1. Use the best performing model for predictions\")\n",
    "print(\"2. Consider ensemble methods for improved performance\")\n",
    "print(\"3. Focus on the most important features for feature engineering\")\n",
    "print(\"4. Monitor model performance on new data\")\n",
    "print(\"5. Regular retraining with updated data\")\n",
    "print(\"6. Consider business context when interpreting results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 ذخیره نتایج ارزیابی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation results\n",
    "import json\n",
    "\n",
    "evaluation_summary = {\n",
    "    'best_model': {\n",
    "        'name': best_model_name if best_model_name else 'None',\n",
    "        'r2_score': float(best_result['r2']) if best_model_name else 0.0,\n",
    "        'rmse': float(best_result['rmse']) if best_model_name else 0.0,\n",
    "        'mae': float(best_result['mae']) if best_model_name else 0.0\n",
    "    },\n",
    "    'model_performance': performance_analysis,\n",
    "    'cv_stability': cv_stability,\n",
    "    'feature_importance_summary': {}\n",
    "}\n",
    "\n",
    "# Add feature importance data\n",
    "if feature_importance_summary:\n",
    "    for model_name, importance in feature_importance_summary.items():\n",
    "        evaluation_summary['feature_importance_summary'][model_name] = {\n",
    "            feature: float(score) for feature, score in importance.items()\n",
    "        }\n",
    "\n",
    "with open('../results/evaluation_summary.json', 'w') as f:\n",
    "    json.dump(evaluation_summary, f, indent=2)\n",
    "\n",
    "print(\"💾 Evaluation results saved to 'results/evaluation_summary.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 خلاصه نهایی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 Final Evaluation Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Models evaluated: {len(results)}\")\n",
    "print(f\"Best model: {best_model_name if best_model_name else 'None'}\")\n",
    "print(f\"Best R² score: {trainer.best_score:.4f if trainer.best_score else 'N/A'}\")\n",
    "print(f\"Feature importance analyzed: {len(feature_importance_summary)} models\")\n",
    "print(f\"Evaluation report created: ✅\")\n",
    "print(f\"Residual analysis performed: ✅\")\n",
    "print(f\"Learning curves generated: ✅\")\n",
    "print(f\"Model comparison completed: ✅\")\n",
    "\n",
    "print(f\"\\n📁 Generated Files:\")\n",
    "print(f\"  - results/evaluation_report.txt\")\n",
    "print(f\"  - results/evaluation_summary.json\")\n",
    "print(f\"  - Various evaluation plots and visualizations\")\n",
    "\n",
    "print(f\"\\n🎉 Comprehensive evaluation completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
     "language_info": {
    "codemirror_mode": {
     "name": "ipython",
     "version": 3
    },
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "nbconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": "3.8.5"
   }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
