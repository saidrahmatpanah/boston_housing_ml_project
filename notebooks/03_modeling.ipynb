{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¤– Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…Ø§Ø´ÛŒÙ† Ù„Ø±Ù†ÛŒÙ†Ú¯ - Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÙˆØ³ØªÙˆÙ†\n",
    "\n",
    "Ø§ÛŒÙ† Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ø§Ù…Ù„ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ù…Ø§Ø´ÛŒÙ† Ù„Ø±Ù†ÛŒÙ†Ú¯ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù‚ÛŒÙ…Øª Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø§Ø³Øª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "import joblib\n",
    "from data_loader import BostonHousingDataLoader\n",
    "from preprocessing import DataPreprocessor\n",
    "from models import ModelTrainer\n",
    "\n",
    "# Load data\n",
    "loader = BostonHousingDataLoader()\n",
    "features, target, feature_names = loader.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "preprocessor = DataPreprocessor(scaler_type='standard')\n",
    "features_clean = preprocessor.handle_outliers(features, strategy='clip')\n",
    "X_train, X_test, y_train, y_test = preprocessor.split_data(features_clean, target)\n",
    "X_train_scaled, X_test_scaled = preprocessor.scale_features(X_train, X_test)\n",
    "\n",
    "print(f\"ğŸ“Š Data loaded and preprocessed successfully!\")\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Testing set: {X_test_scaled.shape}\")\n",
    "print(f\"Features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¤– Ù…Ø¹Ø±ÙÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø§Ø´ÛŒÙ† Ù„Ø±Ù†ÛŒÙ†Ú¯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Get available models\n",
    "available_models = trainer.get_models()\n",
    "print(\"ğŸ¤– Available Machine Learning Models:\")\n",
    "for i, (name, model) in enumerate(available_models.items(), 1):\n",
    "    print(f\"{i:2d}. {name:<20} - {type(model).__name__}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Total models available: {len(available_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\"ğŸš€ Training all models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = trainer.train_models(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f\"\\nâœ… Training completed!\")\n",
    "print(f\"Models trained: {len(results)}\")\n",
    "print(f\"Best model: {trainer.best_model.__class__.__name__ if trainer.best_model else 'None'}\")\n",
    "print(f\"Best RÂ² score: {trainer.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ø®Ù„Ø§ØµÙ‡ Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary\n",
    "performance_summary = []\n",
    "for model_name, result in results.items():\n",
    "    performance_summary.append({\n",
    "        'Model': model_name,\n",
    "        'RÂ²': result['r2'],\n",
    "        'RMSE': result['rmse'],\n",
    "        'MAE': result['mae'],\n",
    "        'CV_RÂ²_Mean': result['cv_mean'],\n",
    "        'CV_RÂ²_Std': result['cv_std']\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_summary)\n",
    "performance_df = performance_df.sort_values('RÂ²', ascending=False)\n",
    "\n",
    "print(\"ğŸ“Š Model Performance Summary (Sorted by RÂ²):\")\n",
    "display(performance_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "trainer.plot_model_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ† ØªØ­Ù„ÛŒÙ„ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model information\n",
    "best_model_info = trainer.get_best_model_info()\n",
    "\n",
    "if best_model_info:\n",
    "    print(\"ğŸ† Best Model Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Model Name: {best_model_info['name']}\")\n",
    "    print(f\"RÂ² Score: {best_model_info['r2_score']:.4f}\")\n",
    "    print(f\"RMSE: {best_model_info['metrics']['rmse']:.4f}\")\n",
    "    print(f\"MAE: {best_model_info['metrics']['mae']:.4f}\")\n",
    "    print(f\"CV RÂ² Mean: {best_model_info['metrics']['cv_mean']:.4f}\")\n",
    "    print(f\"CV RÂ² Std: {best_model_info['metrics']['cv_std']:.4f}\")\n",
    "    \n",
    "    # Show model type\n",
    "    model_type = type(best_model_info['model']).__name__\n",
    "    print(f\"Model Type: {model_type}\")\n",
    "    \n",
    "    # Performance category\n",
    "    r2_score = best_model_info['r2_score']\n",
    "    if r2_score >= 0.8:\n",
    "        performance = \"Excellent ğŸ¯\"\n",
    "    elif r2_score >= 0.6:\n",
    "        performance = \"Good ğŸ‘\"\n",
    "    elif r2_score >= 0.4:\n",
    "        performance = \"Fair âš ï¸\"\n",
    "    else:\n",
    "        performance = \"Poor âŒ\"\n",
    "    \n",
    "    print(f\"Performance Category: {performance}\")\n",
    "else:\n",
    "    print(\"âŒ No best model found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” ØªØ­Ù„ÛŒÙ„ Ø§Ù‡Ù…ÛŒØª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'XGBoost', 'Gradient Boosting', 'Decision Tree']\n",
    "feature_importance_results = {}\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in trainer.models:\n",
    "        print(f\"\\nğŸ” Getting feature importance for {model_name}...\")\n",
    "        importance = trainer.get_feature_importance(model_name, feature_names)\n",
    "        if importance:\n",
    "            feature_importance_results[model_name] = importance\n",
    "            print(f\"âœ… Feature importance obtained for {model_name}\")\n",
    "            \n",
    "            # Show top 5 features\n",
    "            print(f\"Top 5 features for {model_name}:\")\n",
    "            for i, (feature, score) in enumerate(list(importance.items())[:5], 1):\n",
    "                print(f\"  {i}. {feature}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"âŒ Could not get feature importance for {model_name}\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ {model_name} not found in trained models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for best tree-based model\n",
    "if feature_importance_results:\n",
    "    # Find the best tree-based model\n",
    "    best_tree_model = None\n",
    "    best_tree_score = -1\n",
    "    \n",
    "    for model_name in tree_models:\n",
    "        if model_name in results and model_name in feature_importance_results:\n",
    "            if results[model_name]['r2'] > best_tree_score:\n",
    "                best_tree_score = results[model_name]['r2']\n",
    "                best_tree_model = model_name\n",
    "    \n",
    "    if best_tree_model:\n",
    "        print(f\"\\nğŸ“Š Plotting feature importance for best tree-based model: {best_tree_model}\")\n",
    "        trainer.plot_feature_importance(best_tree_model, top_n=10)\n",
    "    else:\n",
    "        print(\"âŒ No tree-based model with feature importance found!\")\n",
    "else:\n",
    "    print(\"âŒ No feature importance results available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù‡ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "print(\"âš™ï¸ Performing hyperparameter tuning for Random Forest...\")\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_tuning_result = trainer.hyperparameter_tuning(\n",
    "    X_train_scaled, y_train, 'Random Forest', rf_param_grid\n",
    ")\n",
    "\n",
    "if rf_tuning_result:\n",
    "    print(f\"âœ… Random Forest tuning completed!\")\n",
    "    print(f\"Best parameters: {rf_tuning_result['best_params']}\")\n",
    "    print(f\"Best CV score: {rf_tuning_result['best_score']:.4f}\")\n",
    "else:\n",
    "    print(\"âŒ Random Forest tuning failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for XGBoost\n",
    "print(\"\\nâš™ï¸ Performing hyperparameter tuning for XGBoost...\")\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "xgb_tuning_result = trainer.hyperparameter_tuning(\n",
    "    X_train_scaled, y_train, 'XGBoost', xgb_param_grid\n",
    ")\n",
    "\n",
    "if xgb_tuning_result:\n",
    "    print(f\"âœ… XGBoost tuning completed!\")\n",
    "    print(f\"Best parameters: {xgb_tuning_result['best_params']}\")\n",
    "    print(f\"Best CV score: {xgb_tuning_result['best_score']:.4f}\")\n",
    "else:\n",
    "    print(\"âŒ XGBoost tuning failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù‚Ø¨Ù„ Ùˆ Ø¨Ø¹Ø¯ Ø§Ø² Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance before and after tuning\n",
    "print(\"ğŸ“Š Performance Comparison (Before vs After Tuning):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "models_to_compare = ['Random Forest', 'XGBoost']\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    if model_name in results:\n",
    "        # Get original performance\n",
    "        original_r2 = results[model_name]['r2']\n",
    "        original_rmse = results[model_name]['rmse']\n",
    "        \n",
    "        # Get tuned performance (if available)\n",
    "        if model_name in trainer.models:\n",
    "            # Retrain with best parameters to get new metrics\n",
    "            tuned_model = trainer.models[model_name]\n",
    "            y_pred_tuned = tuned_model.predict(X_test_scaled)\n",
    "            \n",
    "            from sklearn.metrics import r2_score, mean_squared_error\n",
    "            tuned_r2 = r2_score(y_test, y_pred_tuned)\n",
    "            tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "            \n",
    "            improvement_r2 = tuned_r2 - original_r2\n",
    "            improvement_rmse = original_rmse - tuned_rmse\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Original_RÂ²': original_r2,\n",
    "                'Tuned_RÂ²': tuned_r2,\n",
    "                'RÂ²_Improvement': improvement_r2,\n",
    "                'Original_RMSE': original_rmse,\n",
    "                'Tuned_RMSE': tuned_rmse,\n",
    "                'RMSE_Improvement': improvement_rmse\n",
    "            })\n",
    "        else:\n",
    "            print(f\"âš ï¸ {model_name} not available for comparison\")\n",
    "\n",
    "if comparison_data:\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\nğŸ“Š Detailed Comparison:\")\n",
    "    display(comparison_df.round(4))\n",
    "    \n",
    "    # Show improvements\n",
    "    print(\"\\nğŸ¯ Summary of Improvements:\")\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        print(f\"{row['Model']}:\")\n",
    "        print(f\"  RÂ²: {row['Original_RÂ²']:.4f} â†’ {row['Tuned_RÂ²']:.4f} ({row['RÂ²_Improvement']:+.4f})\")\n",
    "        print(f\"  RMSE: {row['Original_RMSE']:.4f} â†’ {row['Tuned_RMSE']:.4f} ({row['RMSE_Improvement']:+.4f})\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"âŒ No comparison data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ ØªØ­Ù„ÛŒÙ„ Ø®Ø·Ø§Ù‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction errors for best model\n",
    "if best_model_info:\n",
    "    best_model_name = best_model_info['name']\n",
    "    y_pred_best = results[best_model_name]['y_pred']\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = y_test - y_pred_best\n",
    "    \n",
    "    # Error statistics\n",
    "    print(f\"ğŸ¯ Error Analysis for {best_model_name}:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Mean Error: {errors.mean():.4f}\")\n",
    "    print(f\"Std Error: {errors.std():.4f}\")\n",
    "    print(f\"Min Error: {errors.min():.4f}\")\n",
    "    print(f\"Max Error: {errors.max():.4f}\")\n",
    "    print(f\"Mean Absolute Error: {np.abs(errors).mean():.4f}\")\n",
    "    print(f\"Median Absolute Error: {np.median(np.abs(errors)):.4f}\")\n",
    "    \n",
    "    # Error distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram of errors\n",
    "    ax1.hist(errors, bins=30, alpha=0.7, edgecolor='black', color='lightcoral')\n",
    "    ax1.set_title(f'Error Distribution - {best_model_name}')\n",
    "    ax1.set_xlabel('Prediction Error')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot of actual vs predicted with error bands\n",
    "    ax2.scatter(y_test, y_pred_best, alpha=0.6, color='blue')\n",
    "    ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    ax2.fill_between([y_test.min(), y_test.max()], \n",
    "                     [y_test.min() - errors.std(), y_test.max() - errors.std()],\n",
    "                     [y_test.min() + errors.std(), y_test.max() + errors.std()], \n",
    "                     alpha=0.2, color='red', label=f'Â±1 Std Error ({errors.std():.2f})')\n",
    "    ax2.set_xlabel('Actual Values')\n",
    "    ax2.set_ylabel('Predicted Values')\n",
    "    ax2.set_title(f'Actual vs Predicted - {best_model_name}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for systematic errors\n",
    "    print(f\"\\nğŸ” Systematic Error Analysis:\")\n",
    "    if errors.mean() > 0.1:\n",
    "        print(f\"âš ï¸ Model tends to overpredict (mean error: {errors.mean():.4f})\")\n",
    "    elif errors.mean() < -0.1:\n",
    "        print(f\"âš ï¸ Model tends to underpredict (mean error: {errors.mean():.4f})\")\n",
    "    else:\n",
    "        print(f\"âœ… Model predictions are well-balanced (mean error: {errors.mean():.4f})\")\n",
    "    \n",
    "    # Check error variance\n",
    "    if errors.std() > 5:\n",
    "        print(f\"âš ï¸ High prediction variance (std error: {errors.std():.4f})\")\n",
    "    else:\n",
    "        print(f\"âœ… Predictions are consistent (std error: {errors.std():.4f})\")\n",
    "else:\n",
    "    print(\"âŒ No best model available for error analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ùˆ Ù†ØªØ§ÛŒØ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "if best_model_info:\n",
    "    best_model_name = best_model_info['name']\n",
    "    print(f\"ğŸ’¾ Saving best model: {best_model_name}\")\n",
    "    trainer.save_model(best_model_name, '../models/best_model.pkl')\n",
    "    \n",
    "    # Save all models\n",
    "    print(f\"ğŸ’¾ Saving all trained models...\")\n",
    "    for model_name in results.keys():\n",
    "        trainer.save_model(model_name, f'../models/{model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "    \n",
    "    print(f\"âœ… All models saved successfully!\")\n",
    "else:\n",
    "    print(\"âŒ No best model to save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training results\n",
    "import json\n",
    "\n",
    "training_results = {}\n",
    "for model_name, result in results.items():\n",
    "    training_results[model_name] = {\n",
    "        'r2': float(result['r2']),\n",
    "        'rmse': float(result['rmse']),\n",
    "        'mae': float(result['mae']),\n",
    "        'cv_mean': float(result['cv_mean']),\n",
    "        'cv_std': float(result['cv_std'])\n",
    "    }\n",
    "\n",
    "# Add feature importance\n",
    "if feature_importance_results:\n",
    "    training_results['feature_importance'] = {}\n",
    "    for model_name, importance in feature_importance_results.items():\n",
    "        training_results['feature_importance'][model_name] = {\n",
    "            feature: float(score) for feature, score in importance.items()\n",
    "        }\n",
    "\n",
    "# Add best model info\n",
    "if best_model_info:\n",
    "    training_results['best_model'] = {\n",
    "        'name': best_model_info['name'],\n",
    "        'r2_score': float(best_model_info['r2_score']),\n",
    "        'model_type': type(best_model_info['model']).__name__\n",
    "    }\n",
    "\n",
    "with open('../results/training_results.json', 'w') as f:\n",
    "    json.dump(training_results, f, indent=2)\n",
    "\n",
    "print(\"ğŸ’¾ Training results saved to 'results/training_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ù…Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ Modeling Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total models trained: {len(results)}\")\n",
    "print(f\"Best model: {best_model_info['name'] if best_model_info else 'None'}\")\n",
    "print(f\"Best RÂ² score: {trainer.best_score:.4f if trainer.best_score else 'N/A'}\")\n",
    "print(f\"Models with feature importance: {len(feature_importance_results)}\")\n",
    "print(f\"Hyperparameter tuning performed: {'Random Forest' in results and 'XGBoost' in results}\")\n",
    "\n",
    "print(f\"\\nğŸ† Top 3 Models:\")\n",
    "top_3_models = sorted(results.items(), key=lambda x: x[1]['r2'], reverse=True)[:3]\n",
    "for i, (model_name, result) in enumerate(top_3_models, 1):\n",
    "    print(f\"{i}. {model_name}: RÂ² = {result['r2']:.4f}, RMSE = {result['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nğŸ“ Files generated:\")\n",
    "print(f\"  - models/best_model.pkl\")\n",
    "print(f\"  - models/*.pkl (all models)\")\n",
    "print(f\"  - results/training_results.json\")\n",
    "print(f\"  - Feature importance plots\")\n",
    "print(f\"  - Model comparison plots\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
