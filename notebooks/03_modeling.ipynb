{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🤖 مدل‌سازی ماشین لرنینگ - خانه‌های بوستون\n",
    "\n",
    "این نوت‌بوک شامل آموزش و ارزیابی مدل‌های مختلف ماشین لرنینگ برای پیش‌بینی قیمت خانه‌ها است."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 بارگذاری داده‌های پیش‌پردازش شده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data\n",
    "import joblib\n",
    "from data_loader import BostonHousingDataLoader\n",
    "from preprocessing import DataPreprocessor\n",
    "from models import ModelTrainer\n",
    "\n",
    "# Load data\n",
    "loader = BostonHousingDataLoader()\n",
    "features, target, feature_names = loader.load_data()\n",
    "\n",
    "# Preprocess data\n",
    "preprocessor = DataPreprocessor(scaler_type='standard')\n",
    "features_clean = preprocessor.handle_outliers(features, strategy='clip')\n",
    "X_train, X_test, y_train, y_test = preprocessor.split_data(features_clean, target)\n",
    "X_train_scaled, X_test_scaled = preprocessor.scale_features(X_train, X_test)\n",
    "\n",
    "print(f\"📊 Data loaded and preprocessed successfully!\")\n",
    "print(f\"Training set: {X_train_scaled.shape}\")\n",
    "print(f\"Testing set: {X_test_scaled.shape}\")\n",
    "print(f\"Features: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 معرفی مدل‌های ماشین لرنینگ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model trainer\n",
    "trainer = ModelTrainer()\n",
    "\n",
    "# Get available models\n",
    "available_models = trainer.get_models()\n",
    "print(\"🤖 Available Machine Learning Models:\")\n",
    "for i, (name, model) in enumerate(available_models.items(), 1):\n",
    "    print(f\"{i:2d}. {name:<20} - {type(model).__name__}\")\n",
    "\n",
    "print(f\"\\n📊 Total models available: {len(available_models)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 آموزش مدل‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train all models\n",
    "print(\"🚀 Training all models...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = trainer.train_models(X_train_scaled, y_train, X_test_scaled, y_test)\n",
    "\n",
    "print(f\"\\n✅ Training completed!\")\n",
    "print(f\"Models trained: {len(results)}\")\n",
    "print(f\"Best model: {trainer.best_model.__class__.__name__ if trainer.best_model else 'None'}\")\n",
    "print(f\"Best R² score: {trainer.best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 خلاصه عملکرد مدل‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance summary\n",
    "performance_summary = []\n",
    "for model_name, result in results.items():\n",
    "    performance_summary.append({\n",
    "        'Model': model_name,\n",
    "        'R²': result['r2'],\n",
    "        'RMSE': result['rmse'],\n",
    "        'MAE': result['mae'],\n",
    "        'CV_R²_Mean': result['cv_mean'],\n",
    "        'CV_R²_Std': result['cv_std']\n",
    "    })\n",
    "\n",
    "performance_df = pd.DataFrame(performance_summary)\n",
    "performance_df = performance_df.sort_values('R²', ascending=False)\n",
    "\n",
    "print(\"📊 Model Performance Summary (Sorted by R²):\")\n",
    "display(performance_df.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 نمودار مقایسه مدل‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "trainer.plot_model_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🏆 تحلیل بهترین مدل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best model information\n",
    "best_model_info = trainer.get_best_model_info()\n",
    "\n",
    "if best_model_info:\n",
    "    print(\"🏆 Best Model Analysis:\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"Model Name: {best_model_info['name']}\")\n",
    "    print(f\"R² Score: {best_model_info['r2_score']:.4f}\")\n",
    "    print(f\"RMSE: {best_model_info['metrics']['rmse']:.4f}\")\n",
    "    print(f\"MAE: {best_model_info['metrics']['mae']:.4f}\")\n",
    "    print(f\"CV R² Mean: {best_model_info['metrics']['cv_mean']:.4f}\")\n",
    "    print(f\"CV R² Std: {best_model_info['metrics']['cv_std']:.4f}\")\n",
    "    \n",
    "    # Show model type\n",
    "    model_type = type(best_model_info['model']).__name__\n",
    "    print(f\"Model Type: {model_type}\")\n",
    "    \n",
    "    # Performance category\n",
    "    r2_score = best_model_info['r2_score']\n",
    "    if r2_score >= 0.8:\n",
    "        performance = \"Excellent 🎯\"\n",
    "    elif r2_score >= 0.6:\n",
    "        performance = \"Good 👍\"\n",
    "    elif r2_score >= 0.4:\n",
    "        performance = \"Fair ⚠️\"\n",
    "    else:\n",
    "        performance = \"Poor ❌\"\n",
    "    \n",
    "    print(f\"Performance Category: {performance}\")\n",
    "else:\n",
    "    print(\"❌ No best model found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 تحلیل اهمیت ویژگی‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance for tree-based models\n",
    "tree_models = ['Random Forest', 'XGBoost', 'Gradient Boosting', 'Decision Tree']\n",
    "feature_importance_results = {}\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in trainer.models:\n",
    "        print(f\"\\n🔍 Getting feature importance for {model_name}...\")\n",
    "        importance = trainer.get_feature_importance(model_name, feature_names)\n",
    "        if importance:\n",
    "            feature_importance_results[model_name] = importance\n",
    "            print(f\"✅ Feature importance obtained for {model_name}\")\n",
    "            \n",
    "            # Show top 5 features\n",
    "            print(f\"Top 5 features for {model_name}:\")\n",
    "            for i, (feature, score) in enumerate(list(importance.items())[:5], 1):\n",
    "                print(f\"  {i}. {feature}: {score:.4f}\")\n",
    "        else:\n",
    "            print(f\"❌ Could not get feature importance for {model_name}\")\n",
    "    else:\n",
    "        print(f\"⚠️ {model_name} not found in trained models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance for best tree-based model\n",
    "if feature_importance_results:\n",
    "    # Find the best tree-based model\n",
    "    best_tree_model = None\n",
    "    best_tree_score = -1\n",
    "    \n",
    "    for model_name in tree_models:\n",
    "        if model_name in results and model_name in feature_importance_results:\n",
    "            if results[model_name]['r2'] > best_tree_score:\n",
    "                best_tree_score = results[model_name]['r2']\n",
    "                best_tree_model = model_name\n",
    "    \n",
    "    if best_tree_model:\n",
    "        print(f\"\\n📊 Plotting feature importance for best tree-based model: {best_tree_model}\")\n",
    "        trainer.plot_feature_importance(best_tree_model, top_n=10)\n",
    "    else:\n",
    "        print(\"❌ No tree-based model with feature importance found!\")\n",
    "else:\n",
    "    print(\"❌ No feature importance results available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ بهینه‌سازی هیپرپارامترها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "print(\"⚙️ Performing hyperparameter tuning for Random Forest...\")\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "rf_tuning_result = trainer.hyperparameter_tuning(\n",
    "    X_train_scaled, y_train, 'Random Forest', rf_param_grid\n",
    ")\n",
    "\n",
    "if rf_tuning_result:\n",
    "    print(f\"✅ Random Forest tuning completed!\")\n",
    "    print(f\"Best parameters: {rf_tuning_result['best_params']}\")\n",
    "    print(f\"Best CV score: {rf_tuning_result['best_score']:.4f}\")\n",
    "else:\n",
    "    print(\"❌ Random Forest tuning failed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for XGBoost\n",
    "print(\"\\n⚙️ Performing hyperparameter tuning for XGBoost...\")\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "xgb_tuning_result = trainer.hyperparameter_tuning(\n",
    "    X_train_scaled, y_train, 'XGBoost', xgb_param_grid\n",
    ")\n",
    "\n",
    "if xgb_tuning_result:\n",
    "    print(f\"✅ XGBoost tuning completed!\")\n",
    "    print(f\"Best parameters: {xgb_tuning_result['best_params']}\")\n",
    "    print(f\"Best CV score: {xgb_tuning_result['best_score']:.4f}\")\n",
    "else:\n",
    "    print(\"❌ XGBoost tuning failed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 مقایسه قبل و بعد از بهینه‌سازی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare performance before and after tuning\n",
    "print(\"📊 Performance Comparison (Before vs After Tuning):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "models_to_compare = ['Random Forest', 'XGBoost']\n",
    "comparison_data = []\n",
    "\n",
    "for model_name in models_to_compare:\n",
    "    if model_name in results:\n",
    "        # Get original performance\n",
    "        original_r2 = results[model_name]['r2']\n",
    "        original_rmse = results[model_name]['rmse']\n",
    "        \n",
    "        # Get tuned performance (if available)\n",
    "        if model_name in trainer.models:\n",
    "            # Retrain with best parameters to get new metrics\n",
    "            tuned_model = trainer.models[model_name]\n",
    "            y_pred_tuned = tuned_model.predict(X_test_scaled)\n",
    "            \n",
    "            from sklearn.metrics import r2_score, mean_squared_error\n",
    "            tuned_r2 = r2_score(y_test, y_pred_tuned)\n",
    "            tuned_rmse = np.sqrt(mean_squared_error(y_test, y_pred_tuned))\n",
    "            \n",
    "            improvement_r2 = tuned_r2 - original_r2\n",
    "            improvement_rmse = original_rmse - tuned_rmse\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Original_R²': original_r2,\n",
    "                'Tuned_R²': tuned_r2,\n",
    "                'R²_Improvement': improvement_r2,\n",
    "                'Original_RMSE': original_rmse,\n",
    "                'Tuned_RMSE': tuned_rmse,\n",
    "                'RMSE_Improvement': improvement_rmse\n",
    "            })\n",
    "        else:\n",
    "            print(f\"⚠️ {model_name} not available for comparison\")\n",
    "\n",
    "if comparison_data:\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(\"\\n📊 Detailed Comparison:\")\n",
    "    display(comparison_df.round(4))\n",
    "    \n",
    "    # Show improvements\n",
    "    print(\"\\n🎯 Summary of Improvements:\")\n",
    "    for _, row in comparison_df.iterrows():\n",
    "        print(f\"{row['Model']}:\")\n",
    "        print(f\"  R²: {row['Original_R²']:.4f} → {row['Tuned_R²']:.4f} ({row['R²_Improvement']:+.4f})\")\n",
    "        print(f\"  RMSE: {row['Original_RMSE']:.4f} → {row['Tuned_RMSE']:.4f} ({row['RMSE_Improvement']:+.4f})\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"❌ No comparison data available!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 تحلیل خطاها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction errors for best model\n",
    "if best_model_info:\n",
    "    best_model_name = best_model_info['name']\n",
    "    y_pred_best = results[best_model_name]['y_pred']\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = y_test - y_pred_best\n",
    "    \n",
    "    # Error statistics\n",
    "    print(f\"🎯 Error Analysis for {best_model_name}:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Mean Error: {errors.mean():.4f}\")\n",
    "    print(f\"Std Error: {errors.std():.4f}\")\n",
    "    print(f\"Min Error: {errors.min():.4f}\")\n",
    "    print(f\"Max Error: {errors.max():.4f}\")\n",
    "    print(f\"Mean Absolute Error: {np.abs(errors).mean():.4f}\")\n",
    "    print(f\"Median Absolute Error: {np.median(np.abs(errors)):.4f}\")\n",
    "    \n",
    "    # Error distribution\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Histogram of errors\n",
    "    ax1.hist(errors, bins=30, alpha=0.7, edgecolor='black', color='lightcoral')\n",
    "    ax1.set_title(f'Error Distribution - {best_model_name}')\n",
    "    ax1.set_xlabel('Prediction Error')\n",
    "    ax1.set_ylabel('Frequency')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot of actual vs predicted with error bands\n",
    "    ax2.scatter(y_test, y_pred_best, alpha=0.6, color='blue')\n",
    "    ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "    ax2.fill_between([y_test.min(), y_test.max()], \n",
    "                     [y_test.min() - errors.std(), y_test.max() - errors.std()],\n",
    "                     [y_test.min() + errors.std(), y_test.max() + errors.std()], \n",
    "                     alpha=0.2, color='red', label=f'±1 Std Error ({errors.std():.2f})')\n",
    "    ax2.set_xlabel('Actual Values')\n",
    "    ax2.set_ylabel('Predicted Values')\n",
    "    ax2.set_title(f'Actual vs Predicted - {best_model_name}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check for systematic errors\n",
    "    print(f\"\\n🔍 Systematic Error Analysis:\")\n",
    "    if errors.mean() > 0.1:\n",
    "        print(f\"⚠️ Model tends to overpredict (mean error: {errors.mean():.4f})\")\n",
    "    elif errors.mean() < -0.1:\n",
    "        print(f\"⚠️ Model tends to underpredict (mean error: {errors.mean():.4f})\")\n",
    "    else:\n",
    "        print(f\"✅ Model predictions are well-balanced (mean error: {errors.mean():.4f})\")\n",
    "    \n",
    "    # Check error variance\n",
    "    if errors.std() > 5:\n",
    "        print(f\"⚠️ High prediction variance (std error: {errors.std():.4f})\")\n",
    "    else:\n",
    "        print(f\"✅ Predictions are consistent (std error: {errors.std():.4f})\")\n",
    "else:\n",
    "    print(\"❌ No best model available for error analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 ذخیره مدل‌ها و نتایج"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model\n",
    "if best_model_info:\n",
    "    best_model_name = best_model_info['name']\n",
    "    print(f\"💾 Saving best model: {best_model_name}\")\n",
    "    trainer.save_model(best_model_name, '../models/best_model.pkl')\n",
    "    \n",
    "    # Save all models\n",
    "    print(f\"💾 Saving all trained models...\")\n",
    "    for model_name in results.keys():\n",
    "        trainer.save_model(model_name, f'../models/{model_name.lower().replace(\" \", \"_\")}.pkl')\n",
    "    \n",
    "    print(f\"✅ All models saved successfully!\")\n",
    "else:\n",
    "    print(\"❌ No best model to save!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training results\n",
    "import json\n",
    "\n",
    "training_results = {}\n",
    "for model_name, result in results.items():\n",
    "    training_results[model_name] = {\n",
    "        'r2': float(result['r2']),\n",
    "        'rmse': float(result['rmse']),\n",
    "        'mae': float(result['mae']),\n",
    "        'cv_mean': float(result['cv_mean']),\n",
    "        'cv_std': float(result['cv_std'])\n",
    "    }\n",
    "\n",
    "# Add feature importance\n",
    "if feature_importance_results:\n",
    "    training_results['feature_importance'] = {}\n",
    "    for model_name, importance in feature_importance_results.items():\n",
    "        training_results['feature_importance'][model_name] = {\n",
    "            feature: float(score) for feature, score in importance.items()\n",
    "        }\n",
    "\n",
    "# Add best model info\n",
    "if best_model_info:\n",
    "    training_results['best_model'] = {\n",
    "        'name': best_model_info['name'],\n",
    "        'r2_score': float(best_model_info['r2_score']),\n",
    "        'model_type': type(best_model_info['model']).__name__\n",
    "    }\n",
    "\n",
    "with open('../results/training_results.json', 'w') as f:\n",
    "    json.dump(training_results, f, indent=2)\n",
    "\n",
    "print(\"💾 Training results saved to 'results/training_results.json'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 خلاصه مدل‌سازی"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📋 Modeling Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total models trained: {len(results)}\")\n",
    "print(f\"Best model: {best_model_info['name'] if best_model_info else 'None'}\")\n",
    "print(f\"Best R² score: {trainer.best_score:.4f if trainer.best_score else 'N/A'}\")\n",
    "print(f\"Models with feature importance: {len(feature_importance_results)}\")\n",
    "print(f\"Hyperparameter tuning performed: {'Random Forest' in results and 'XGBoost' in results}\")\n",
    "\n",
    "print(f\"\\n🏆 Top 3 Models:\")\n",
    "top_3_models = sorted(results.items(), key=lambda x: x[1]['r2'], reverse=True)[:3]\n",
    "for i, (model_name, result) in enumerate(top_3_models, 1):\n",
    "    print(f\"{i}. {model_name}: R² = {result['r2']:.4f}, RMSE = {result['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\n📁 Files generated:\")\n",
    "print(f\"  - models/best_model.pkl\")\n",
    "print(f\"  - models/*.pkl (all models)\")\n",
    "print(f\"  - results/training_results.json\")\n",
    "print(f\"  - Feature importance plots\")\n",
    "print(f\"  - Model comparison plots\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
