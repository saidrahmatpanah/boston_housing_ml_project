{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“Š ØªØ­Ù„ÛŒÙ„ Ø§Ú©ØªØ´Ø§ÙÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¨ÙˆØ³ØªÙˆÙ†\n",
    "\n",
    "Ø§ÛŒÙ† Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ø´Ø§Ù…Ù„ ØªØ­Ù„ÛŒÙ„ Ø¬Ø§Ù…Ø¹ Ùˆ Ø§Ú©ØªØ´Ø§ÙÛŒ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Boston Housing Ø§Ø³Øª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¥ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data using our custom loader\n",
    "from data_loader import BostonHousingDataLoader\n",
    "\n",
    "loader = BostonHousingDataLoader()\n",
    "features, target, feature_names = loader.load_data()\n",
    "\n",
    "print(f\"ğŸ“Š Dataset loaded successfully!\")\n",
    "print(f\"Shape: {features.shape}\")\n",
    "print(f\"Features: {len(feature_names)}\")\n",
    "print(f\"Target: {target.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Ø¨Ø±Ø±Ø³ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù¾Ø§ÛŒÙ‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic dataset information\n",
    "info = loader.get_data_info()\n",
    "print(\"ğŸ“‹ Dataset Information:\")\n",
    "for key, value in info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"ğŸ“‹ First 5 rows of features:\")\n",
    "display(features.head())\n",
    "\n",
    "print(\"\\nğŸ“‹ First 5 rows of target:\")\n",
    "display(target.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š Ø¢Ù…Ø§Ø± ØªÙˆØµÛŒÙÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary of features\n",
    "print(\"ğŸ“Š Statistical Summary of Features:\")\n",
    "display(features.describe())\n",
    "\n",
    "print(\"\\nğŸ“Š Statistical Summary of Target:\")\n",
    "display(target.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ” Ø¨Ø±Ø±Ø³ÛŒ Ø§Ù†ÙˆØ§Ø¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data types information\n",
    "print(\"ğŸ” Data Types:\")\n",
    "print(features.dtypes)\n",
    "\n",
    "print(\"\\nğŸ” Target Data Type:\")\n",
    "print(target.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš¨ Ø¨Ø±Ø±Ø³ÛŒ Ù…Ù‚Ø§Ø¯ÛŒØ± Ú¯Ù…â€ŒØ´Ø¯Ù‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = features.isnull().sum()\n",
    "missing_percentage = (missing_values / len(features)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Missing_Percentage': missing_percentage\n",
    "})\n",
    "\n",
    "print(\"ğŸš¨ Missing Values Analysis:\")\n",
    "display(missing_df[missing_df['Missing_Count'] > 0])\n",
    "\n",
    "if missing_df['Missing_Count'].sum() == 0:\n",
    "    print(\"âœ… No missing values found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ ØªÙˆØ²ÛŒØ¹ Ù…ØªØºÛŒØ± Ù‡Ø¯Ù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot target variable distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1.hist(target, bins=30, alpha=0.7, edgecolor='black', color='skyblue')\n",
    "ax1.set_title('Distribution of House Prices (MEDV)')\n",
    "ax1.set_xlabel('Price (in $1000s)')\n",
    "ax1.set_ylabel('Frequency')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "ax2.boxplot(target, patch_artist=True, boxprops=dict(facecolor='lightgreen'))\n",
    "ax2.set_title('Box Plot of House Prices')\n",
    "ax2.set_ylabel('Price (in $1000s)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics\n",
    "print(f\"ğŸ“Š Target Variable Statistics:\")\n",
    "print(f\"  Mean: {target.mean():.2f}\")\n",
    "print(f\"  Median: {target.median():.2f}\")\n",
    "print(f\"  Std: {target.std():.2f}\")\n",
    "print(f\"  Min: {target.min():.2f}\")\n",
    "print(f\"  Max: {target.max():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ØªÙˆØ²ÛŒØ¹ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of all features\n",
    "n_features = len(feature_names)\n",
    "n_cols = 3\n",
    "n_rows = (n_features + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    if i < len(axes):\n",
    "        axes[i].hist(features[feature], bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[i].set_title(f'Distribution of {feature}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for i in range(n_features, len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”— Ù…Ø§ØªØ±ÛŒØ³ Ù‡Ù…Ø¨Ø³ØªÚ¯ÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix including target\n",
    "data_with_target = pd.concat([features, target], axis=1)\n",
    "correlation_matrix = data_with_target.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(\n",
    "    correlation_matrix, \n",
    "    mask=mask, \n",
    "    annot=True, \n",
    "    cmap='coolwarm', \n",
    "    center=0,\n",
    "    square=True,\n",
    "    fmt='.2f'\n",
    ")\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show top correlations with target\n",
    "target_correlations = correlation_matrix[target.name].sort_values(ascending=False)\n",
    "print(\"ğŸ”— Top correlations with target variable:\")\n",
    "display(target_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“Š ØªØ­Ù„ÛŒÙ„ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…Ù‡Ù…"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature descriptions\n",
    "descriptions = loader.get_feature_descriptions()\n",
    "\n",
    "# Show top 5 features by correlation with target\n",
    "top_features = target_correlations[1:6]  # Exclude target itself\n",
    "\n",
    "print(\"ğŸ† Top 5 Most Important Features:\")\n",
    "for i, (feature, corr) in enumerate(top_features.items(), 1):\n",
    "    print(f\"{i}. {feature} (Correlation: {corr:.3f})\")\n",
    "    print(f\"   Description: {descriptions.get(feature, 'No description available')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ù¾Ø±Ø§Ú©Ù†Ø¯Ú¯ÛŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots for top features vs target\n",
    "top_5_features = top_features.index[:5]\n",
    "n_cols = 2\n",
    "n_rows = (len(top_5_features) + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top_5_features):\n",
    "    if i < len(axes):\n",
    "        axes[i].scatter(features[feature], target, alpha=0.6, color='blue')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('House Price (MEDV)')\n",
    "        axes[i].set_title(f'{feature} vs House Price')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide empty subplots\n",
    "for i in range(len(top_5_features), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš¨ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for all features to identify outliers\n",
    "plt.figure(figsize=(15, 8))\n",
    "features.boxplot(figsize=(15, 8))\n",
    "plt.title('Box Plots of All Features (Outlier Detection)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Feature Values')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count outliers using IQR method\n",
    "outlier_counts = {}\n",
    "for feature in feature_names:\n",
    "    Q1 = features[feature].quantile(0.25)\n",
    "    Q3 = features[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = features[(features[feature] < lower_bound) | (features[feature] > upper_bound)]\n",
    "    outlier_counts[feature] = len(outliers)\n",
    "\n",
    "print(\"ğŸš¨ Outlier Counts (IQR method):\")\n",
    "for feature, count in sorted(outlier_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    if count > 0:\n",
    "        percentage = (count / len(features)) * 100\n",
    "        print(f\"  {feature}: {count} outliers ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ ØªØ­Ù„ÛŒÙ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“‹ Data Exploration Summary:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset Shape: {features.shape}\")\n",
    "print(f\"Number of Features: {len(feature_names)}\")\n",
    "print(f\"Number of Samples: {len(features)}\")\n",
    "print(f\"Target Variable: {target.name}\")\n",
    "print(f\"Missing Values: {features.isnull().sum().sum()}\")\n",
    "print(f\"Data Types: {features.dtypes.unique()}\")\n",
    "print(f\"\\nTop 3 Features by Correlation:\")\n",
    "for i, (feature, corr) in enumerate(top_features.head(3).items(), 1):\n",
    "    print(f\"  {i}. {feature}: {corr:.3f}\")\n",
    "print(f\"\\nTarget Statistics:\")\n",
    "print(f\"  Mean: {target.mean():.2f}\")\n",
    "print(f\"  Std: {target.std():.2f}\")\n",
    "print(f\"  Range: {target.max() - target.min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save exploration results\n",
    "exploration_results = {\n",
    "    'dataset_shape': features.shape,\n",
    "    'feature_names': list(feature_names),\n",
    "    'target_name': target.name,\n",
    "    'missing_values': features.isnull().sum().to_dict(),\n",
    "    'target_correlations': target_correlations.to_dict(),\n",
    "    'outlier_counts': outlier_counts,\n",
    "    'target_statistics': {\n",
    "        'mean': float(target.mean()),\n",
    "        'std': float(target.std()),\n",
    "        'min': float(target.min()),\n",
    "        'max': float(target.max()),\n",
    "        'median': float(target.median())\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../results/exploration_results.json', 'w') as f:\n",
    "    json.dump(exploration_results, f, indent=2)\n",
    "\n",
    "print(\"ğŸ’¾ Exploration results saved to 'results/exploration_results.json'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
