{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔧 پیش‌پردازش داده‌های خانه‌های بوستون\n",
    "\n",
    "این نوت‌بوک شامل مراحل پیش‌پردازش داده‌ها مانند تمیز کردن، مدیریت outliers، مقیاس‌بندی و تقسیم داده‌ها است."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 بارگذاری داده‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "from data_loader import BostonHousingDataLoader\n",
    "from preprocessing import DataPreprocessor\n",
    "\n",
    "loader = BostonHousingDataLoader()\n",
    "features, target, feature_names = loader.load_data()\n",
    "\n",
    "print(f\"📊 Dataset loaded successfully!\")\n",
    "print(f\"Original shape: {features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 بررسی کیفیت داده‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessor\n",
    "preprocessor = DataPreprocessor(scaler_type='standard')\n",
    "\n",
    "# Check missing values\n",
    "missing_info = preprocessor.check_missing_values(features)\n",
    "print(\"🚨 Missing Values Analysis:\")\n",
    "print(f\"Total missing: {missing_info['total_missing']}\")\n",
    "if missing_info['total_missing'] > 0:\n",
    "    print(\"\\nMissing values per column:\")\n",
    "    for col, count in missing_info['missing_per_column'].items():\n",
    "        if count > 0:\n",
    "            print(f\"  {col}: {count} ({missing_info['missing_percentage'][col]:.2f}%)\")\n",
    "else:\n",
    "    print(\"✅ No missing values found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚨 شناسایی و مدیریت Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check outliers using IQR method\n",
    "outlier_info = preprocessor.check_outliers(features, method='iqr', threshold=1.5)\n",
    "print(\"🚨 Outlier Analysis (IQR method):\")\n",
    "print(f\"Features with outliers: {len([k for k, v in outlier_info.items() if v['count'] > 0])}\")\n",
    "\n",
    "# Show features with most outliers\n",
    "outlier_summary = [(k, v['count'], v['percentage']) for k, v in outlier_info.items() if v['count'] > 0]\n",
    "outlier_summary.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "if outlier_summary:\n",
    "    print(\"\\nTop features with outliers:\")\n",
    "    for feature, count, percentage in outlier_summary[:5]:\n",
    "        print(f\"  {feature}: {count} outliers ({percentage:.2f}%)\")\n",
    "else:\n",
    "    print(\"✅ No outliers detected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize outliers for top features\n",
    "if outlier_summary:\n",
    "    top_outlier_features = [feature for feature, _, _ in outlier_summary[:6]]\n",
    "    n_cols = 2\n",
    "    n_rows = (len(top_outlier_features) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(top_outlier_features):\n",
    "        if i < len(axes):\n",
    "            # Box plot\n",
    "            axes[i].boxplot(features[feature], patch_artist=True, \n",
    "                           boxprops=dict(facecolor='lightblue'))\n",
    "            axes[i].set_title(f'Outliers in {feature}')\n",
    "            axes[i].set_ylabel(feature)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(top_outlier_features), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 تمیز کردن داده‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers using clipping strategy\n",
    "print(\"🧹 Handling outliers using clipping strategy...\")\n",
    "features_clean = preprocessor.handle_outliers(features, strategy='clip')\n",
    "\n",
    "# Compare before and after\n",
    "print(f\"\\n📊 Data shape comparison:\")\n",
    "print(f\"  Before cleaning: {features.shape}\")\n",
    "print(f\"  After cleaning: {features_clean.shape}\")\n",
    "\n",
    "# Check outliers after cleaning\n",
    "outlier_info_after = preprocessor.check_outliers(features_clean, method='iqr', threshold=1.5)\n",
    "outliers_after = sum([v['count'] for v in outlier_info_after.values()])\n",
    "print(f\"  Outliers before: {sum([v['count'] for v in outlier_info.values()])}\")\n",
    "print(f\"  Outliers after: {outliers_after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 تحلیل توزیع قبل و بعد از تمیز کردن"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions before and after cleaning for top outlier features\n",
    "if outlier_summary:\n",
    "    top_features = [feature for feature, _, _ in outlier_summary[:4]]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, feature in enumerate(top_features):\n",
    "        if i < len(axes):\n",
    "            # Before cleaning\n",
    "            axes[i].hist(features[feature], bins=20, alpha=0.7, \n",
    "                        label='Before', color='red', edgecolor='black')\n",
    "            # After cleaning\n",
    "            axes[i].hist(features_clean[feature], bins=20, alpha=0.7, \n",
    "                        label='After', color='green', edgecolor='black')\n",
    "            axes[i].set_title(f'Distribution of {feature}')\n",
    "            axes[i].set_xlabel(feature)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 تقسیم داده‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "print(\"📈 Splitting data into training and testing sets...\")\n",
    "X_train, X_test, y_train, y_test = preprocessor.split_data(features_clean, target)\n",
    "\n",
    "print(f\"\\n📊 Split results:\")\n",
    "print(f\"  Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(features_clean)*100:.1f}%)\")\n",
    "print(f\"  Testing set: {X_test.shape[0]} samples ({X_test.shape[0]/len(features_clean)*100:.1f}%)\")\n",
    "print(f\"  Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 مقیاس‌بندی ویژگی‌ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using StandardScaler\n",
    "print(\"🔄 Scaling features using StandardScaler...\")\n",
    "X_train_scaled, X_test_scaled = preprocessor.scale_features(X_train, X_test)\n",
    "\n",
    "print(f\"✅ Scaling completed!\")\n",
    "print(f\"  Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"  Testing set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare distributions before and after scaling\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Select a few features for visualization\n",
    "sample_features = feature_names[:4]\n",
    "\n",
    "for i, feature in enumerate(sample_features):\n",
    "    if i < len(axes):\n",
    "        # Before scaling\n",
    "        axes[i].hist(X_train[feature], bins=20, alpha=0.7, \n",
    "                    label='Before Scaling', color='blue', edgecolor='black')\n",
    "        # After scaling\n",
    "        feature_idx = list(feature_names).index(feature)\n",
    "        axes[i].hist(X_train_scaled[:, feature_idx], bins=20, alpha=0.7, \n",
    "                    label='After Scaling', color='orange', edgecolor='black')\n",
    "        axes[i].set_title(f'Scaling Effect on {feature}')\n",
    "        axes[i].set_xlabel('Feature Value')\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 تست انواع مختلف Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different scaler types\n",
    "scaler_types = ['standard', 'robust', 'minmax']\n",
    "scaled_data = {}\n",
    "\n",
    "for scaler_type in scaler_types:\n",
    "    print(f\"\\n🔧 Testing {scaler_type.title()} Scaler...\")\n",
    "    temp_preprocessor = DataPreprocessor(scaler_type=scaler_type)\n",
    "    X_train_temp, X_test_temp = temp_preprocessor.scale_features(X_train, X_test)\n",
    "    \n",
    "    # Calculate statistics\n",
    "    train_mean = X_train_temp.mean(axis=0).mean()\n",
    "    train_std = X_train_temp.std(axis=0).mean()\n",
    "    train_min = X_train_temp.min(axis=0).mean()\n",
    "    train_max = X_train_temp.max(axis=0).mean()\n",
    "    \n",
    "    print(f\"  Mean: {train_mean:.4f}\")\n",
    "    print(f\"  Std: {train_std:.4f}\")\n",
    "    print(f\"  Min: {train_min:.4f}\")\n",
    "    print(f\"  Max: {train_max:.4f}\")\n",
    "    \n",
    "    scaled_data[scaler_type] = {\n",
    "        'X_train': X_train_temp,\n",
    "        'X_test': X_test_temp,\n",
    "        'stats': {'mean': train_mean, 'std': train_std, 'min': train_min, 'max': train_max}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 مقایسه انواع Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare scaler effects\n",
    "scaler_names = list(scaled_data.keys())\n",
    "stats_names = ['mean', 'std', 'min', 'max']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, stat in enumerate(stats_names):\n",
    "    values = [scaled_data[scaler]['stats'][stat] for scaler in scaler_names]\n",
    "    \n",
    "    bars = axes[i].bar(scaler_names, values, color=['skyblue', 'lightgreen', 'lightcoral'], alpha=0.7)\n",
    "    axes[i].set_title(f'{stat.upper()} Comparison Across Scalers')\n",
    "    axes[i].set_ylabel(stat.upper())\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, values):\n",
    "        height = bar.get_height()\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                    f'{value:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 ویژگی‌های چندجمله‌ای"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features\n",
    "print(\"🎯 Creating polynomial features (degree 2)...\")\n",
    "X_train_poly = preprocessor.create_polynomial_features(X_train, degree=2)\n",
    "X_test_poly = preprocessor.create_polynomial_features(X_test, degree=2)\n",
    "\n",
    "print(f\"✅ Polynomial features created!\")\n",
    "print(f\"  Original features: {X_train.shape[1]}\")\n",
    "print(f\"  Polynomial features: {X_train_poly.shape[1]}\")\n",
    "print(f\"  Additional features: {X_train_poly.shape[1] - X_train.shape[1]}\")\n",
    "\n",
    "# Show some polynomial feature names\n",
    "print(f\"\\n📋 Sample polynomial feature names:\")\n",
    "for i, feature in enumerate(X_train_poly.columns[:10]):\n",
    "    print(f\"  {i+1}. {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 خلاصه پیش‌پردازش"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get preprocessing summary\n",
    "summary = preprocessor.get_preprocessing_summary(features, features_clean)\n",
    "\n",
    "print(\"📋 Preprocessing Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in summary.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(f\"\\n🔧 Scaler used: {preprocessor.scaler_type}\")\n",
    "print(f\"📊 Final data shapes:\")\n",
    "print(f\"  Training: {X_train_scaled.shape}\")\n",
    "print(f\"  Testing: {X_test_scaled.shape}\")\n",
    "print(f\"  Target training: {y_train.shape}\")\n",
    "print(f\"  Target testing: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 ذخیره داده‌های پیش‌پردازش شده"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save preprocessed data\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Save scaled data\n",
    "joblib.dump(X_train_scaled, '../results/X_train_scaled.pkl')\n",
    "joblib.dump(X_test_scaled, '../results/X_test_scaled.pkl')\n",
    "joblib.dump(y_train, '../results/y_train.pkl')\n",
    "joblib.dump(y_test, '../results/y_test.pkl')\n",
    "\n",
    "# Save preprocessing info\n",
    "preprocessing_info = {\n",
    "    'scaler_type': preprocessor.scaler_type,\n",
    "    'feature_names': list(feature_names),\n",
    "    'original_shape': features.shape,\n",
    "    'cleaned_shape': features_clean.shape,\n",
    "    'training_samples': X_train_scaled.shape[0],\n",
    "    'testing_samples': X_test_scaled.shape[0],\n",
    "    'features_count': X_train_scaled.shape[1],\n",
    "    'outlier_info': outlier_info,\n",
    "    'missing_values': missing_info\n",
    "}\n",
    "\n",
    "with open('../results/preprocessing_info.json', 'w') as f:\n",
    "    json.dump(preprocessing_info, f, indent=2)\n",
    "\n",
    "print(\"💾 Preprocessed data saved successfully!\")\n",
    "print(\"  - X_train_scaled.pkl\")\n",
    "print(\"  - X_test_scaled.pkl\")\n",
    "print(\"  - y_train.pkl\")\n",
    "print(\"  - y_test.pkl\")\n",
    "print(\"  - preprocessing_info.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
